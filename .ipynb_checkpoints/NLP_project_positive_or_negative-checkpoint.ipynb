{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    #1. Tokenize\n",
    "    #text_tokens = word_tokenize(data.lower())\n",
    "    text_tokens = word_tokenize(data) # removed lower for tagging\n",
    "    \n",
    "    #2.Remove puncs\n",
    "    text_tokens = [t for t in text_tokens if t.isalpha()]\n",
    "    \n",
    "    #3. stop words\n",
    "    text_tokens = [t for t in text_tokens if not t in stop_words]\n",
    "    \n",
    "    #4. Lemma\n",
    "    text_tokens = [lem.lemmatize(t) for t in text_tokens]\n",
    "    \n",
    "    #5 Join\n",
    "    return \" \".join(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2           79582\n",
       "positive    42133\n",
       "negative    34345\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment = df.Sentiment.apply(lambda x : \"positive\" if x in [3,4] else ('negative' if x in [0,1] else 2))\n",
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    42133\n",
       "negative    34345\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Sentiment']!=2]\n",
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good for the goose</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the gander , some of which occasionally amuses...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amuses</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences sentiment\n",
       "0  A series of escapades demonstrating the adage ...  negative\n",
       "1                                 good for the goose  positive\n",
       "2                                               good  positive\n",
       "3  the gander , some of which occasionally amuses...  negative\n",
       "4                                             amuses  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Phrase','Sentiment']]\n",
    "df.columns = ['sentences', 'sentiment']\n",
    "df = df.reset_index(drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences_2\"] = df['sentences'].apply(cleaning) #Model\n",
    "#df['sentences_3'] = df['sentences_2'].apply(lambda x : x.split()) #PoST\n",
    "#df['sentences_4'] = df['sentences_3'].apply(lambda x : nltk.pos_tag(x)) #tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aan</th>\n",
       "      <th>abagnale</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbass</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaliyah  aan  abagnale  abandon  abandoned  abbass  abbott  \\\n",
       "0    0        0    0         0        0          0       0       0   \n",
       "1    0        0    0         0        0          0       0       0   \n",
       "2    0        0    0         0        0          0       0       0   \n",
       "3    0        0    0         0        0          0       0       0   \n",
       "4    0        0    0         0        0          0       0       0   \n",
       "\n",
       "   abbreviated  abc  ...  ziyi  zoe  zombie  zone  zoning  zoolander  zoom  \\\n",
       "0            0    0  ...     0    0       0     0       0          0     0   \n",
       "1            0    0  ...     0    0       0     0       0          0     0   \n",
       "2            0    0  ...     0    0       0     0       0          0     0   \n",
       "3            0    0  ...     0    0       0     0       0          0     0   \n",
       "4            0    0  ...     0    0       0     0       0          0     0   \n",
       "\n",
       "   zucker  zwick  zzzzzzzzz  \n",
       "0       0      0          0  \n",
       "1       0      0          0  \n",
       "2       0      0          0  \n",
       "3       0      0          0  \n",
       "4       0      0          0  \n",
       "\n",
       "[5 rows x 13323 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[\"sentences_2\"]\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "vectorizer = CountVectorizer().fit(X)\n",
    "X_count = vectorizer.transform(X)\n",
    "count_df_X = pd.DataFrame( X_count.toarray(),columns = vectorizer.get_feature_names() )\n",
    "count_df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aan</th>\n",
       "      <th>abagnale</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbass</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaliyah  aan  abagnale  abandon  abandoned  abbass  abbott  \\\n",
       "0  0.0      0.0  0.0       0.0      0.0        0.0     0.0     0.0   \n",
       "1  0.0      0.0  0.0       0.0      0.0        0.0     0.0     0.0   \n",
       "2  0.0      0.0  0.0       0.0      0.0        0.0     0.0     0.0   \n",
       "3  0.0      0.0  0.0       0.0      0.0        0.0     0.0     0.0   \n",
       "4  0.0      0.0  0.0       0.0      0.0        0.0     0.0     0.0   \n",
       "\n",
       "   abbreviated  abc  ...  ziyi  zoe  zombie  zone  zoning  zoolander  zoom  \\\n",
       "0          0.0  0.0  ...   0.0  0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "1          0.0  0.0  ...   0.0  0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "2          0.0  0.0  ...   0.0  0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "3          0.0  0.0  ...   0.0  0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "4          0.0  0.0  ...   0.0  0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "\n",
       "   zucker  zwick  zzzzzzzzz  \n",
       "0     0.0    0.0        0.0  \n",
       "1     0.0    0.0        0.0  \n",
       "2     0.0    0.0        0.0  \n",
       "3     0.0    0.0        0.0  \n",
       "4     0.0    0.0        0.0  \n",
       "\n",
       "[5 rows x 13323 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[\"sentences_2\"]\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer().fit(X)\n",
    "X_tf_idf = tf_idf_vectorizer.transform(X)\n",
    "\n",
    "tf_idf_df_X = pd.DataFrame(X_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())\n",
    "tf_idf_df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = count_df_X  # count vectorize df X\n",
    "X2 = tf_idf_df_X # tfidf vectorize df X\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "yorj = y.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y\n",
    "\n",
    "y = pd.DataFrame(y,columns = ['sentinent'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/me/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5683 1102]\n",
      " [ 779 7732]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      6785\n",
      "           1       0.88      0.91      0.89      8511\n",
      "\n",
      "    accuracy                           0.88     15296\n",
      "   macro avg       0.88      0.87      0.87     15296\n",
      "weighted avg       0.88      0.88      0.88     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model=LogisticRegression().fit(X_train,y_train)\n",
    "y_pred=log_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "# y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from xgboost import XGBClassifier\n",
    "# xgb = XGBClassifier()\n",
    "# xgb_params = {\"n_estimators\": [50, 100], \"subsample\":[0.5,1], \"max_depth\":[3,7], \"learning_rate\":[0.1,0.3]}\n",
    "# xgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 3, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
    "\n",
    "# print(xgb_cv_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5560 1225]\n",
      " [1009 7502]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.83      6785\n",
      "           1       0.86      0.88      0.87      8511\n",
      "\n",
      "    accuracy                           0.85     15296\n",
      "   macro avg       0.85      0.85      0.85     15296\n",
      "weighted avg       0.85      0.85      0.85     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_naive = MultinomialNB().fit(X_train,y_train)\n",
    "y_pred=model_naive.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5560 1225]\n",
      " [1009 7502]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.83      6785\n",
      "           1       0.86      0.88      0.87      8511\n",
      "\n",
      "    accuracy                           0.85     15296\n",
      "   macro avg       0.85      0.85      0.85     15296\n",
      "weighted avg       0.85      0.85      0.85     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_naive = MultinomialNB().fit(X_train,y_train.values.ravel())\n",
    "y_pred=model_naive.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=cleaning)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3741 3044]\n",
      " [1655 6856]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.55      0.61      6785\n",
      "    positive       0.69      0.81      0.74      8511\n",
      "\n",
      "    accuracy                           0.69     15296\n",
      "   macro avg       0.69      0.68      0.68     15296\n",
      "weighted avg       0.69      0.69      0.69     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xorj = df[\"sentences_2\"]\n",
    "yorj = df[\"sentiment\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xorj_train, Xorj_test, yorj_train, yorj_test = train_test_split(Xorj, yorj, test_size = 0.2, random_state = 42)\n",
    "\n",
    "pipeline.fit(Xorj_train,yorj_train)\n",
    "y_pred = pipeline.predict(Xorj_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(yorj_test, y_pred))\n",
    "print(classification_report(yorj_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4907 1878]\n",
      " [1042 7469]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.72      0.77      6785\n",
      "    positive       0.80      0.88      0.84      8511\n",
      "\n",
      "    accuracy                           0.81     15296\n",
      "   macro avg       0.81      0.80      0.80     15296\n",
      "weighted avg       0.81      0.81      0.81     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    #1. Tokenize\n",
    "    #text_tokens = word_tokenize(data.lower())\n",
    "    text_tokens = word_tokenize(data) # removed lower for tagging\n",
    "    \n",
    "    #2.Remove puncs\n",
    "    text_tokens = [t for t in text_tokens if t.isalpha()]\n",
    "    \n",
    "    #3. stop words\n",
    "    text_tokens = [t for t in text_tokens if not t in stop_words]\n",
    "    \n",
    "    #4. Lemma\n",
    "    text_tokens = [lem.lemmatize(t) for t in text_tokens]\n",
    "    \n",
    "    #5 Join\n",
    "    return \" \".join(text_tokens)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=cleaning)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', RandomForestClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "Xorj = df[\"sentences_2\"]\n",
    "yorj = df[\"sentiment\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xorj_train, Xorj_test, yorj_train, yorj_test = train_test_split(Xorj, yorj, test_size = 0.2, random_state = 42)\n",
    "\n",
    "pipeline.fit(Xorj_train,yorj_train)\n",
    "y_pred = pipeline.predict(Xorj_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(yorj_test, y_pred))\n",
    "print(classification_report(yorj_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3741 3044]\n",
      " [1655 6856]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.55      0.61      6785\n",
      "    positive       0.69      0.81      0.74      8511\n",
      "\n",
      "    accuracy                           0.69     15296\n",
      "   macro avg       0.69      0.68      0.68     15296\n",
      "weighted avg       0.69      0.69      0.69     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    #1. Tokenize\n",
    "    #text_tokens = word_tokenize(data.lower())\n",
    "    text_tokens = word_tokenize(data) # removed lower for tagging\n",
    "    \n",
    "    #2.Remove puncs\n",
    "    text_tokens = [t for t in text_tokens if t.isalpha()]\n",
    "    \n",
    "    #3. stop words\n",
    "    text_tokens = [t for t in text_tokens if not t in stop_words]\n",
    "    \n",
    "    #4. Lemma\n",
    "    text_tokens = [lem.lemmatize(t) for t in text_tokens]\n",
    "    \n",
    "    #5 Join\n",
    "    return \" \".join(text_tokens)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=cleaning)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', XGBClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "Xorj = df[\"sentences_2\"]\n",
    "yorj = df[\"sentiment\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xorj_train, Xorj_test, yorj_train, yorj_test = train_test_split(Xorj, yorj, test_size = 0.2, random_state = 42)\n",
    "\n",
    "pipeline.fit(Xorj_train,yorj_train)\n",
    "y_pred = pipeline.predict(Xorj_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(yorj_test, y_pred))\n",
    "print(classification_report(yorj_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2442 4343]\n",
      " [1848 6663]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.36      0.44      6785\n",
      "    positive       0.61      0.78      0.68      8511\n",
      "\n",
      "    accuracy                           0.60     15296\n",
      "   macro avg       0.59      0.57      0.56     15296\n",
      "weighted avg       0.59      0.60      0.58     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    #1. Tokenize\n",
    "    #text_tokens = word_tokenize(data.lower())\n",
    "    text_tokens = word_tokenize(data) # removed lower for tagging\n",
    "    \n",
    "    #2.Remove puncs\n",
    "    text_tokens = [t for t in text_tokens if t.isalpha()]\n",
    "    \n",
    "    #3. stop words\n",
    "    text_tokens = [t for t in text_tokens if not t in stop_words]\n",
    "    \n",
    "    #4. Lemma\n",
    "    text_tokens = [lem.lemmatize(t) for t in text_tokens]\n",
    "    \n",
    "    #5 Join\n",
    "    return \" \".join(text_tokens)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=cleaning)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "Xorj = df[\"sentences_2\"]\n",
    "yorj = df[\"sentiment\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xorj_train, Xorj_test, yorj_train, yorj_test = train_test_split(Xorj, yorj, test_size = 0.2, random_state = 42)\n",
    "\n",
    "pipeline.fit(Xorj_train,yorj_train)\n",
    "y_pred = pipeline.predict(Xorj_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(yorj_test, y_pred))\n",
    "print(classification_report(yorj_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2391 4394]\n",
      " [1808 6703]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.35      0.44      6785\n",
      "    positive       0.60      0.79      0.68      8511\n",
      "\n",
      "    accuracy                           0.59     15296\n",
      "   macro avg       0.59      0.57      0.56     15296\n",
      "weighted avg       0.59      0.59      0.57     15296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    #1. Tokenize\n",
    "    #text_tokens = word_tokenize(data.lower())\n",
    "    text_tokens = word_tokenize(data) # removed lower for tagging\n",
    "    \n",
    "    #2.Remove puncs\n",
    "    text_tokens = [t for t in text_tokens if t.isalpha()]\n",
    "    \n",
    "    #3. stop words\n",
    "    text_tokens = [t for t in text_tokens if not t in stop_words]\n",
    "    \n",
    "    #4. Lemma\n",
    "    text_tokens = [lem.lemmatize(t) for t in text_tokens]\n",
    "    \n",
    "    #5 Join\n",
    "    return \" \".join(text_tokens)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer,TfidfVectorizer, CountVectorizer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', TfidfVectorizer(analyzer=cleaning)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "X = df[\"sentences_2\"]\n",
    "y = df[\"sentiment\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        A series escapade demonstrating adage good goo...\n",
       "1                                               good goose\n",
       "2                                                     good\n",
       "3        gander occasionally amuses none amount much story\n",
       "4                                                   amuses\n",
       "                               ...                        \n",
       "76473    quietly suggesting sadness obsession beneath H...\n",
       "76474                                    sadness obsession\n",
       "76475                                              sadness\n",
       "76476                             forced avuncular chortle\n",
       "76477                                    avuncular chortle\n",
       "Name: sentences_2, Length: 76478, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        negative\n",
       "1        positive\n",
       "2        positive\n",
       "3        negative\n",
       "4        positive\n",
       "           ...   \n",
       "76473    negative\n",
       "76474    negative\n",
       "76475    negative\n",
       "76476    negative\n",
       "76477    positive\n",
       "Name: sentiment, Length: 76478, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
